{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News Detection with Python\n",
    "\n",
    "This project aims at detecting fake news thorugh machine learning some real data. The dataset used is available at: https://data-flair.training/blogs/advanced-python-project-detecting-fake-news/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data file\n",
    "news = pd.read_csv('news.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "8476                        You Can Smell Hillary’s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label  \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "875    It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6335 entries, 8476 to 4330\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   6335 non-null   object\n",
      " 1   text    6335 non-null   object\n",
      " 2   label   6335 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 198.0+ KB\n"
     ]
    }
   ],
   "source": [
    "news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>3164</td>\n",
       "      <td>3164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>3171</td>\n",
       "      <td>3171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title  text\n",
       "label             \n",
       "FAKE    3164  3164\n",
       "REAL    3171  3171"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real and fake pieces of news are well balanced in number.  Next, we study text length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     3171.000000\n",
       "mean      5292.160202\n",
       "std       4348.288284\n",
       "min         43.000000\n",
       "25%       2729.500000\n",
       "50%       4683.000000\n",
       "75%       6829.500000\n",
       "max      44039.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[news['label']=='REAL']['text'].map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      3164.000000\n",
       "mean       4121.046460\n",
       "std        5680.232733\n",
       "min           1.000000\n",
       "25%        1283.500000\n",
       "50%        2558.000000\n",
       "75%        5027.000000\n",
       "max      115372.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[news['label']=='FAKE']['text'].map(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake news has higher varaince of text length, i.e., extremely short or long texts are more often fake. This is also reflceted by the respective minimum and maximum values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use CountVectorizer to convert a text into a vector. It cannot handle two different columns of texts. Therefore, we will merge the 'title' and 'text' columns into one. Dropping the 'title' column altogether may eliminate valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging title and text into one text\n",
    "\n",
    "news['text'] = news['title']+' '+news['text']\n",
    "del news['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert label into 0 and 1, which correspond respectively to real and fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label conversion\n",
    "\n",
    "news['label'] = news['label'].map({'REAL': 0,'FAKE': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear Daniel Greenfield...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy U....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "8476   You Can Smell Hillary’s Fear Daniel Greenfield...      1\n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...      1\n",
       "3608   Kerry to go to Paris in gesture of sympathy U....      0\n",
       "10142  Bernie supporters on Twitter erupt in anger ag...      1\n",
       "875    The Battle of New York: Why This Primary Matte...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use natural language toolkit to clean up the texts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rakibur\n",
      "[nltk_data]     Rahman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "def text_preprocess(text):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns cleaned text in as string\n",
    "    \"\"\"\n",
    "    # removing punctuation\n",
    "    no_punc = [char for char in text if char not in string.punctuation]\n",
    "\n",
    "    # rejoining the characters to form the string\n",
    "    no_punc = ''.join(no_punc)\n",
    "    \n",
    "    # removing stopwords\n",
    "    clean = [word for word in no_punc.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    # rejoining characters to form string\n",
    "    clean = ' '.join(clean)\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be seen a posteriori that stemming the words would lower the performance of our models. We do not go for it. Now we execute text pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1620.5 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "news['text'] = news['text'].apply(text_preprocess)\n",
    "\n",
    "t2 = time.time()\n",
    "print('Time taken: {:.1f} sec'.format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we do the train-test splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "train_test_split(news['text'], news['label'],test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation with default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the various classifiers to be expored\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = {'Logistic Regression': LogisticRegression(),\n",
    "       'Passive-Aggressive': PassiveAggressiveClassifier(),\n",
    "       'K Neighbors': KNeighborsClassifier(), \n",
    "       'Support Vector': SVC(), \n",
    "       'Multinomial NB': MultinomialNB(),\n",
    "       'Decision Tree': DecisionTreeClassifier(), \n",
    "       'Random Forest': RandomForestClassifier(), \n",
    "       'XG Boost': XGBClassifier(use_label_encoder=False, eval_metric='auc')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for computing cross-validated training accuracy \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cross_val_report(clf_,X_,y_,cv_):\n",
    "    '''This function gives cross-validated training accuracy for given classifier'''\n",
    "    ti = time.time()\n",
    "    # pipeline is created to vectorize, Tfidf transform and then classify \n",
    "    pipeline = Pipeline([('bow',CountVectorizer()), ('tfidf',TfidfTransformer()), ('clf',clf_)])\n",
    "    acc_list = cross_val_score(pipeline, X_, y_, cv=cv_, scoring='accuracy')\n",
    "    mean_ = round(100*acc_list.mean(),1)\n",
    "    err_ = round(100*np.sqrt(acc_list.std()/(cv_-1)),1)\n",
    "    accuracy_ = str(mean_)+' $\\pm$ '+str(err_)\n",
    "    \n",
    "    tf = time.time()\n",
    "    time_ = round(tf-ti,1)\n",
    "    \n",
    "    return accuracy_, time_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training accuracy is computed for all classfiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression took 28.9 sec\n",
      "Passive-Aggressive took 23.5 sec\n",
      "K Neighbors took 24.8 sec\n",
      "Support Vector took 431.9 sec\n",
      "Multinomial NB took 22.7 sec\n",
      "Decision Tree took 67.8 sec\n",
      "Random Forest took 74.0 sec\n",
      "XG Boost took 302.8 sec\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "time_taken = []\n",
    "\n",
    "for i in range(len(clf)):\n",
    "    accuracy_, time_ = \\\n",
    "    cross_val_report(list(clf.values())[i],X_train,y_train,10)\n",
    "    accuracy.append(accuracy_)\n",
    "    time_taken.append(time_)\n",
    "    print(list(clf.keys())[i]+' took {} sec'.format(time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for cross validation\n",
    "\n",
    "cross_val_data = {'Train Accuracy (%)': accuracy, 'Time (sec)': time_taken}\n",
    "df1 = pd.DataFrame(index=clf.keys(), data=cross_val_data, columns=cross_val_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Accuracy (%)</th>\n",
       "      <th>Time (sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>91.5 $\\pm$ 3.8</td>\n",
       "      <td>28.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passive-Aggressive</th>\n",
       "      <td>93.8 $\\pm$ 3.0</td>\n",
       "      <td>23.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K Neighbors</th>\n",
       "      <td>84.7 $\\pm$ 3.6</td>\n",
       "      <td>24.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector</th>\n",
       "      <td>93.1 $\\pm$ 3.0</td>\n",
       "      <td>431.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multinomial NB</th>\n",
       "      <td>83.2 $\\pm$ 3.6</td>\n",
       "      <td>22.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>80.7 $\\pm$ 4.1</td>\n",
       "      <td>67.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>90.2 $\\pm$ 2.5</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XG Boost</th>\n",
       "      <td>92.6 $\\pm$ 3.7</td>\n",
       "      <td>302.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Train Accuracy (%)  Time (sec)\n",
       "Logistic Regression     91.5 $\\pm$ 3.8        28.9\n",
       "Passive-Aggressive      93.8 $\\pm$ 3.0        23.5\n",
       "K Neighbors             84.7 $\\pm$ 3.6        24.8\n",
       "Support Vector          93.1 $\\pm$ 3.0       431.9\n",
       "Multinomial NB          83.2 $\\pm$ 3.6        22.7\n",
       "Decision Tree           80.7 $\\pm$ 4.1        67.8\n",
       "Random Forest           90.2 $\\pm$ 2.5        74.0\n",
       "XG Boost                92.6 $\\pm$ 3.7       302.8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of tunable hyperparameters of various classifiers\n",
    "\n",
    "from scipy.stats import expon\n",
    "\n",
    "hp = {'K Neighbors': dict(n_neighbors = list(range(1,31)),\n",
    "                          metric = ['euclidean', 'manhattan', 'minkowski'], \n",
    "                          weights = ['uniform', 'distance']),\n",
    "      'Passive-Aggressive': dict(C = 10.**np.arange(-3, 3),\n",
    "                                 max_iter = list(range(1_000,10_100,100))),\n",
    "      'Logistic Regression': dict(C = 10.**np.arange(-3, 3),\n",
    "                                  penalty = ['l1', 'l2', 'elasticnet']),\n",
    "      'Decision Tree': dict(criterion = ['gini', 'entropy'],\n",
    "                            min_samples_leaf = list(range(1,10)),\n",
    "                            max_depth = list(range(2,14,2))),\n",
    "      'Support Vector': dict(C = expon(scale=100),      # param distribution for SVC \n",
    "                             gamma = expon(scale=.1)),\n",
    "      'Random Forest': dict(n_estimators = [10,50,100,500,1000],\n",
    "                            max_features = ['auto', 'sqrt', 'log2']),\n",
    "      'XG Boost': dict(learning_rate = 0.1*np.arange(1,11), # default = 0.3\n",
    "                       max_depth = list(range(2,31,2)),  # default = 6\n",
    "                       min_child_weight = 10.**np.arange(-2,3), # default = 1\n",
    "                       min_split_loss = 0.1**np.arange(0,5), # default = 0\n",
    "                       colsample_bytree = 0.1*np.arange(1,11)), # default = 1\n",
    "      'Multinomial NB': dict(alpha=[1.0], fit_prior=[True], class_prior=[None])} # default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary for best estimators\n",
    "best_values = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for grid serach/randomized serach of best hyperparameters\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "def best_estimator(clf_name,X_,y_,cv_,rs_=False,n_iter=None):\n",
    "    '''This function finds best estimator for a given classifier'''\n",
    "    ti = time.time()\n",
    "    if rs_:\n",
    "        grid = RandomizedSearchCV(clf[clf_name], param_distributions=hp[clf_name], \n",
    "                                  n_iter=n_iter, scoring='accuracy', n_jobs=-1, cv=cv_)\n",
    "    else:\n",
    "        grid = GridSearchCV(clf[clf_name], param_grid=hp[clf_name], \n",
    "                        scoring='accuracy', n_jobs=-1, cv=cv_)\n",
    "    pipeline = Pipeline([('bow',CountVectorizer()),\n",
    "                         ('tfidf',TfidfTransformer()), \n",
    "                         ('grid',grid)])\n",
    "    pipeline.fit(X_,y_)\n",
    "    \n",
    "    best_score = round(100*grid.best_score_,1)\n",
    "    best_pipeline = Pipeline([('bow',CountVectorizer()),\n",
    "                              ('tfidf',TfidfTransformer()),\n",
    "                              ('best_est',grid.best_estimator_)])\n",
    "    \n",
    "    tf = time.time()\n",
    "    time_ = round(tf-ti,1)\n",
    "    \n",
    "    best_values[clf_name] = [best_score, best_pipeline, time_]\n",
    "    \n",
    "    print(clf_name+' took '+str(time_)+' sec; best train accuracy: '+str(best_score)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression took 20.2 sec; best train accuracy: 93.7%\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning for Logistic Regression\n",
    "\n",
    "best_estimator('Logistic Regression',X_train,y_train,cv_=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passive-Aggressive took 617.4 sec; best train accuracy: 94.0%\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning for Passive-Aggressive\n",
    "\n",
    "best_estimator('Passive-Aggressive',X_train,y_train,cv_=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Neighbors took 1285.2 sec; best train accuracy: 86.1%\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning for K Neighbors\n",
    "\n",
    "best_estimator('K Neighbors',X_train,y_train,cv_=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector took 1061.6 sec; best train accuracy: 93.9%\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning for Support vector\n",
    "\n",
    "best_estimator('Support Vector',X_train,y_train,cv_=10,rs_=True,n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB took 2.6 sec; best train accuracy: 83.4%\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB (no tuning, just default hyperparameters)\n",
    "\n",
    "best_estimator('Multinomial NB',X_train,y_train,cv_=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree took 628.5 sec; best train accuracy: 82.2%\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning for Decision Tree\n",
    "\n",
    "best_estimator('Decision Tree',X_train,y_train,cv_=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest took 1316.9 sec; best train accuracy: 91.5%\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning for Random Forest\n",
    "\n",
    "best_estimator('Random Forest',X_train,y_train,cv_=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XG Boost took 2459.0 sec; best train accuracy: 92.2%\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning for XG Boost\n",
    "\n",
    "best_estimator('XG Boost',X_train,y_train,cv_=10,rs_=True,n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for best training scores\n",
    "\n",
    "def best_list(index):\n",
    "    return [list(best_values.values())[i][index] for i in range(len(best_values))]\n",
    "        \n",
    "best_new = {'Best Score (%)': best_list(0), 'Search Time (sec)': best_list(2)}\n",
    "df2 = pd.DataFrame(index=best_values.keys(), data=best_new, columns=best_new.keys())\n",
    "\n",
    "# inclusion of default performance\n",
    "df_best = pd.concat([df2,df1['Train Accuracy (%)'].apply(lambda i: float(i[:4]))], axis=1)\n",
    "df_best.rename(columns={'Train Accuracy (%)':'Default Accuracy (%)'}, inplace=True)\n",
    "\n",
    "# null accuracy for training data\n",
    "null_train = round(100*max(y_train.mean(),1-y_train.mean()),1)\n",
    "\n",
    "# discard bad classifiers, if any, which give accuracy close to the null value\n",
    "df_best = df_best[df_best['Best Score (%)']>(null_train+5.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Score (%)</th>\n",
       "      <th>Search Time (sec)</th>\n",
       "      <th>Default Accuracy (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>93.7</td>\n",
       "      <td>20.2</td>\n",
       "      <td>91.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passive-Aggressive</th>\n",
       "      <td>94.0</td>\n",
       "      <td>617.4</td>\n",
       "      <td>93.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K Neighbors</th>\n",
       "      <td>86.1</td>\n",
       "      <td>1285.2</td>\n",
       "      <td>84.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector</th>\n",
       "      <td>93.9</td>\n",
       "      <td>1061.6</td>\n",
       "      <td>93.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multinomial NB</th>\n",
       "      <td>83.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>83.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>82.2</td>\n",
       "      <td>628.5</td>\n",
       "      <td>80.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>91.5</td>\n",
       "      <td>1316.9</td>\n",
       "      <td>90.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XG Boost</th>\n",
       "      <td>92.2</td>\n",
       "      <td>2459.0</td>\n",
       "      <td>92.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Best Score (%)  Search Time (sec)  Default Accuracy (%)\n",
       "Logistic Regression            93.7               20.2                  91.5\n",
       "Passive-Aggressive             94.0              617.4                  93.8\n",
       "K Neighbors                    86.1             1285.2                  84.7\n",
       "Support Vector                 93.9             1061.6                  93.1\n",
       "Multinomial NB                 83.4                2.6                  83.2\n",
       "Decision Tree                  82.2              628.5                  80.7\n",
       "Random Forest                  91.5             1316.9                  90.2\n",
       "XG Boost                       92.2             2459.0                  92.6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for evaluating various metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "\n",
    "def test_result(clf_name,X_,y_,X_new,y_new):\n",
    "    '''This function evaluates various performance metrics on test data'''\n",
    "\n",
    "    best_pipeline = best_values[clf_name][1]\n",
    "    best_pipeline.fit(X_,y_)\n",
    "    y_pred = best_pipeline.predict(X_new)\n",
    "    \n",
    "    accuracy = round(accuracy_score(y_new,y_pred),3)\n",
    "    precision = round(precision_score(y_new,y_pred),3)\n",
    "    recall = round(recall_score(y_new,y_pred),3)\n",
    "    f1 = round(f1_score(y_new,y_pred),3)\n",
    "    \n",
    "    return accuracy, precision,recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of performance metrics\n",
    "\n",
    "metrics = {'Accuracy': [], 'Precision': [], 'Recall': [], 'f1 Score': []}\n",
    "for estimator in df_best.index:\n",
    "    accuracy,precision,recall,f1 = test_result(estimator,X_train,y_train,X_test,y_test)\n",
    "    metrics['Accuracy'].append(accuracy)\n",
    "    metrics['Precision'].append(precision)\n",
    "    metrics['Recall'].append(recall)\n",
    "    metrics['f1 Score'].append(f1)\n",
    "\n",
    "df_result = pd.DataFrame(index=df_best.index, data=metrics, columns=metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.940</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passive-Aggressive</th>\n",
       "      <td>0.944</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K Neighbors</th>\n",
       "      <td>0.867</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multinomial NB</th>\n",
       "      <td>0.846</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.796</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.916</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XG Boost</th>\n",
       "      <td>0.919</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Precision  Recall  f1 Score\n",
       "Logistic Regression     0.940      0.914   0.967     0.940\n",
       "Passive-Aggressive      0.944      0.926   0.961     0.943\n",
       "K Neighbors             0.867      0.926   0.790     0.852\n",
       "Support Vector          0.934      0.910   0.959     0.934\n",
       "Multinomial NB          0.846      0.975   0.700     0.815\n",
       "Decision Tree           0.796      0.764   0.837     0.799\n",
       "Random Forest           0.916      0.911   0.917     0.914\n",
       "XG Boost                0.919      0.911   0.922     0.917"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passive Aggressive Classifier is the best performer.\n",
    "Liner Regression is the second best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null accuracy for test data: 51.5%\n"
     ]
    }
   ],
   "source": [
    "# null accuracy for test data\n",
    "\n",
    "null_test = round(100*max(y_test.mean(),1-y_test.mean()),1)\n",
    "print('Null accuracy for test data: {}%'.format(null_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting classifier of 3 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "def voting_(voter_list,vote_type):\n",
    "    '''This function creates a voting classifier of 3 estimators,\n",
    "    which is subsequently trained and tested'''\n",
    "    \n",
    "    # creating voting classifier\n",
    "    estimator_list = []\n",
    "    for clf_name in voter_list:\n",
    "        estimator_list.append((clf_name,best_values[clf_name][1]))\n",
    "    voting_clf = VotingClassifier(estimators=estimator_list,voting=vote_type)\n",
    "    \n",
    "    # training and testing\n",
    "    voting_clf.fit(X_train,y_train)\n",
    "    y_pred = voting_clf.predict(X_test)\n",
    "    \n",
    "    # evaluation of metrics\n",
    "    accuracy = round(accuracy_score(y_test,y_pred),3)\n",
    "    precision = round(precision_score(y_test,y_pred),3)\n",
    "    recall = round(recall_score(y_test,y_pred),3)\n",
    "    f1 = round(f1_score(y_test,y_pred),3)\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the voter list, let us choose the bottom 3 performers (in terms of accuracy with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voter list:  ['K Neighbors', 'Multinomial NB', 'Decision Tree']\n"
     ]
    }
   ],
   "source": [
    "# create voter list\n",
    "voter_list = []\n",
    "cutoff = sorted(df_best['Best Score (%)'])[2]\n",
    "for estimator in df_best.index:\n",
    "    if df_best['Best Score (%)'][estimator] <= cutoff:\n",
    "        voter_list.append(estimator)\n",
    "print('voter list: ', voter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a dataframe of performance metrics\n",
    "\n",
    "voting_metrics = {'Accuracy': [], 'Precision': [], 'Recall': [], 'f1 Score': []}\n",
    "for vote_type in ['hard','soft']:\n",
    "        accuracy, precision, recall, f1 = voting_(voter_list,vote_type)\n",
    "        voting_metrics['Accuracy'].append(accuracy)\n",
    "        voting_metrics['Precision'].append(precision)\n",
    "        voting_metrics['Recall'].append(recall)\n",
    "        voting_metrics['f1 Score'].append(f1)\n",
    "\n",
    "df_voting = pd.DataFrame(index=['Hard Voting','Soft Voting'],\n",
    "                         data=voting_metrics, columns=voting_metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hard Voting</th>\n",
       "      <td>0.892</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soft Voting</th>\n",
       "      <td>0.890</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision  Recall  f1 Score\n",
       "Hard Voting     0.892      0.960   0.811     0.879\n",
       "Soft Voting     0.890      0.931   0.834     0.880"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting classifier has much more improved predictions than individual voters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
